{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the unstable gradients problem with Glorot or He initialization\n",
    "# default is Glorot\n",
    "# if you want to initialize with He initialization use\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "# initialization based on fan_avg vs fan_in\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "# Leaky ReLu prevents neurons from \"dying\" and underfitting (vanishing gradient problem)\n",
    "keras.layers.Dense(10, kernel_initializer=\"he_normal\")\n",
    "keras.layers.LeakyReLU(alpha=0.2) # apply it right after the layer you want to be the leaky one\n",
    "# alpha = leak rate, ~ probability of coming back to life\n",
    "# for SELU (self normalizing layer, also prevents leaking it's good one...)\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization, the initialization techniques stop the function \n",
    "# from having gradient problems at the beginning, but they can come back later\n",
    "# adding normalization operation just before or after the activation function resolves this\n",
    "# this removes the need for things like dropout or early stopping\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")   \n",
    "])\n",
    "\n",
    "# you can also add the normalization before the layer if you add the activation after the layer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\")\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\")\n",
    "    keras.layers.Dense(10, activation=\"softmax\")   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also avoid exploding gradient problem with gradient clipping\n",
    "# keep gradients between a threshold after training, clip them down if they go over\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) clips every gradient vector value to a weight between -1 and 1\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0) a better way to do it is with clipnorm so it doesn't change the angle\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning, reusing the layers of a pre trained NN\n",
    "model_a = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_b_on_a = keras.models.Sequential(model_a.layers[:-1]) adding sequential layer onto model a\n",
    "model_b_on_a.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "# additional training will also train model_a's original layers, to avoid this make a clone/copy\n",
    "model_a_clone = keras.models.clone(model_a)\n",
    "model_a_clone.set_weights(model_a.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the old layers to stay the same\n",
    "for layer in model_b_on_a[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# now when you run the whole thing, your new layers will update their weights, and the old one\n",
    "model_b_on_a.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model_b_on_a.fit(x_train_b, y_train_b, epochs=4, validation_data=(x_valid_b, y_valid_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now unfreeze the frozen layers\n",
    "for layer in model_b_on_a[:-1]:\n",
    "    layer.trainable = True\n",
    "# reduce learning rate to preserve your weights\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "# now refit the whole thing\n",
    "model_b_on_a.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_b_on_a.fit(x_train_b, y_train_b, epochs=4, validation_data=(x_valid_b, y_valid_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "# momentum, picks up speed as it goes in one direction, unlike SGD which has regular steps\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "# nesterov accelerator gradient, measures gradient of the local position \n",
    "# in direction of momentum instead of at current location\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "# Adagrad is good for simple problems because it avoids overfitting, \n",
    "# over time it will decay too much so avoid it for deep networks\n",
    "# accumulator is the initial value of the division term, \n",
    "# epsilon is the constant added to the accumulator to avoid division by zero\n",
    "optimizer = tf.keras.optimizers.Adagrad(lr=0.001, initial_accumulator_value=0.1, epsilon=1e-07)\n",
    "# Use RMSProp for more complex problems, it has an exponential \n",
    "# decay in the division accumulator (it won't grow so fast)\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "# Adam = combo of rmsprop plus momentum, this is the most popular one for deep networks\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "# Nadam is the same as adam but it uses the nesterov trick of calculating ahead of the current position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power scheduling : learning rate is linear compared to epochs\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
    "\n",
    "# exponential decay learning rate drops by a factor of 10 every 20 steps\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "\n",
    "exponential_decay_optimizer = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# now we pass it a callback\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_optimizer)\n",
    "\n",
    "# now we can use it\n",
    "\n",
    "history = model.fit(x_train_scaled, y_train, epochs=4, validation_data=(x_valid_b, y_valid_b), callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a constant learning rate based on the epochs\n",
    "# use this function just like exponential_decay_fn above\n",
    "\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "# this scheduler will multiply lr by 0.5 whenever the best validation doesn't improve for 5 epochs consecutively \n",
    "\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# you can define a learning rate based on one of Keras's schedules\n",
    "\n",
    "s = 20*len(x_train) // 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization l1(), l2(), l1_l2()\n",
    "\n",
    "layer = keras.layers.Dense(100, \n",
    "                           activation=\"elu\", \n",
    "                           kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using partial to avoid repeat calls to function inputs, give function defaults you want\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          )\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "import numpy as np\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# MC Dropout, even better\n",
    "\n",
    "y_probas = np.stack([model(x_test_scaled, training=True) for sample in range(100)])\n",
    "\n",
    "y_proba = y_probas.mean(axis=10)\n",
    "\n",
    "# better way to implement\n",
    "\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "    \n",
    "# max norm regularization, instead of adding a term it justs constrains the weights of the neurons\n",
    "# to use with convolutional layers we need to set max_norm(1., axis =[0, 1, 2])\n",
    "keras.layers.Dense(100, \n",
    "                   activation=\"elu\", \n",
    "                   kernel_initializer=\"he_normal\",\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
