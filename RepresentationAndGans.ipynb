{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undercomplete autoencoder, because encoder has less features\n",
    "# than its inputs, it must learn the most important ones to output correctly\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
    "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))\n",
    "\n",
    "# training the encoder\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train, epochs=50)\n",
    "codings = encoder.predict(x_train) # same dataset is input and targets\n",
    "\n",
    "# autoencoders find another plane to project the data onto, similar to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked encoder for fashion_mnist\n",
    "\n",
    "# for a deeper network add lecun normalization\n",
    "\n",
    "stacked_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation = \"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\"),\n",
    "])\n",
    "\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28*28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28,28])\n",
    "])\n",
    "\n",
    "# better to use binary crossentropy then  mse because \n",
    "# we are doing classification not regression\n",
    "\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder]) \n",
    "stacked_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.5))\n",
    "history = stacked_ae.fit(x_train, x_train, epochs=10, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization the compressed/decoded reconstructions\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def show_reconstructions(model, n_images=5):\n",
    "    reconstructions = model.predict(x_valid[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images*1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(x_valid[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "\n",
    "show_reconstructions(stacked_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders aren't good for visualization, best when combined with another method for this\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "x_valid_compressed = stacked_encoder.predict(x_valid)\n",
    "tsne = TSNE()\n",
    "x_valid_2D = tsne.fit_transform(x_valid_compressed)\n",
    "\n",
    "# plot the dataset\n",
    "\n",
    "plt.scatter(x_valid2D[:, 0], x_valid2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tying the decoder weights to the encoder weights to speed up training\n",
    "\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            initializer=\"zeros\",\n",
    "            shape = [self.dense.input_shape[-1]]\n",
    "        )\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)\n",
    "    \n",
    "# using the layer\n",
    "\n",
    "dense_1 = keras.layers.Dense(100, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(30, activation=\"selu\")\n",
    "\n",
    "tied_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1 ,activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = keras.models..Sequential([tied_encoder, tied_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional autoencoder for working with images\n",
    "\n",
    "conv_encoder = keras.models.Sequential([\n",
    "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool(pool_size=2),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2)\n",
    "])\n",
    "\n",
    "conv_decoder= keras.models.Sequential([\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        32, \n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        padding=\"valid\",\n",
    "        activation=\"selu\",\n",
    "        input_shape=[3, 3, 64]\n",
    "    ),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        16, \n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"selu\"\n",
    "    ),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        1, \n",
    "        kernel_size=3,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"sigmoid\"\n",
    "    ),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recurrent encoder for sequences\n",
    "\n",
    "recurrent_encoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]),\n",
    "    keras.layers.LSTM(30)\n",
    "])\n",
    "\n",
    "recurrent_decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]),\n",
    "    keras.layers.LSTM(30)\n",
    "])\n",
    "\n",
    "recurrent_decoder = keras.models.Sequential([\n",
    "    keras.layers.RepeatVector(28, input_shape=[30]),\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(28, activation=\"sigmoid\"))\n",
    "])\n",
    "recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can add dropout to a model to force it to learn useful features\n",
    "# remember only train with dropout, don't fit\n",
    "\n",
    "dropout_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\")\n",
    "])\n",
    "\n",
    "dropout_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28*28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse autoencoder, less neurons, each one has more info\n",
    "\n",
    "sparse_l1_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    keras.layers.ActivityRegularization(l1=1e-3) # punishes it the further from zero it gets\n",
    "])                                               # but since it is also punished if it can't\n",
    "                                                 # predict, it is forced to output something\n",
    "\n",
    "sparse_l2_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n",
    "    keras.layers.Dense(28 * 28, activation = \"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler, punish if it strays from target activation\n",
    "\n",
    "k = keras.backend\n",
    "\n",
    "kl_divergence = keras.losses.kullback_leibler_divergence\n",
    "\n",
    "class KLDivergenceRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, weight, target=0.1):\n",
    "        self.weight = weight\n",
    "        self.target = target\n",
    "    def __call__(self, inputs):\n",
    "        mean_activities = k.mean(inputs, axis=0)\n",
    "        return self.weight * (\n",
    "            kl_divergence(self.target, mean_activities) + \\\n",
    "            kl_divergence(1. - self.target, 1. - mean_activies)\n",
    "        )\n",
    "    \n",
    "kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\n",
    "sparse_kl_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n",
    "])\n",
    "\n",
    "sparse_kl_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a variational autoencoder\n",
    "# instead of a fixed encoding, it creates a mean, std encoding from training\n",
    "# then when you use it later it samples from this distribution\n",
    "\n",
    "k = keras.backend\n",
    "\n",
    "class Sampling(keras.layers.layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return k.random_normal(tf.shape(log_var)) * k.exp(log_var / 2) + mean\n",
    "\n",
    "# encoder\n",
    "codings_size = 10\n",
    "inputs = keras.layers.Input(shape=[28, 28])\n",
    "z = keras.layers.Flatten()(inputs)\n",
    "z = keras.layers.Dense(150, activation=\"selu\")(z)\n",
    "z = keras.layers.Dense(100, activation=\"selu\")(z)\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z) # actually better to use log of variance than std, faster\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = keras.Model([\n",
    "    inputs = [inputs],\n",
    "    outputs = [codings_mean, codings_log_var, codings]\n",
    "])\n",
    "\n",
    "# decoder\n",
    "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "x = keras.layers.Dense(100, activation=\"selu\")(decoder_inputs)\n",
    "x = keras.layers.Dense(150, activation=\"selu\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "\n",
    "# adding it together\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# add the loss function\n",
    "latent_loss = -0.5 * k.sum(1 + codings_log_var - k.exp(codings_log_var) - k.square(codings_mean), axis=-1)\n",
    "variational_ae.add_loss(k.mean(latent_loss) / 784.)\n",
    "variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# training\n",
    "\n",
    "history = variational_ae.fit(x_train, x_train, epochs=50, batch_size=128, validation_data=[x_valid, x_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating images from a random distribution using decoder\n",
    "\n",
    "codings = tf.random.normal(shape=[12, codings_size])\n",
    "images = variational_decoder(codings).numpy()\n",
    "\n",
    "# semantic interpolation, creating an intermediate between images\n",
    "\n",
    "codings_grid = tf.reshape(codings, [1,2,3,4, codings_size]) # 12 codings to 3x4 grid\n",
    "larger_grid = tf.image.resize(codings_grid, size=[5, 7]) # resize to 5x7\n",
    "interpolated_codings = tf.reshape(larger_grid, [-1, codings_size])\n",
    "images = variational_decoder(interpolated_codings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple GAN\n",
    "\n",
    "# generator is a decoder\n",
    "# discriminator is a binary classifier\n",
    "\n",
    "codings_size = 30\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[codings_size]),\n",
    "    keras.layers.Dense(150, activation=\"selu\"),\n",
    "    keras.layers.Dense(28 * 28, activation=\"selu\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(150, activation=\"selu\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "# compilation, discriminator can compile with a normal loss function, \n",
    "# generator only trained with gan model, don't compile beforehand\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# need to write a custom training loop\n",
    "\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        for x_batch in dataset:\n",
    "            # training discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            x_fake_and_real = tf.concat([generated_images, x_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(x_fake_and_real, y1)\n",
    "            # training generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y2)\n",
    "            \n",
    "# calling it\n",
    "\n",
    "train_gan(gan, dataset, batch_size, codings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep convolutional GAN\n",
    "# replace pooling with strided convolutions in the discriminator\n",
    "# and transposed convolutions in the generator\n",
    "# use batch normalization in both, except in generator output and discriminator input\n",
    "# no FC layers in deeper architechtures\n",
    "# use ReLU in all layers in generator except output which uses tanh\n",
    "# use leaky ReLU in discriminator for all layers\n",
    "\n",
    "codings_size = 100\n",
    "\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(7 * 7 * 128, input_shape = [codings_size]),\n",
    "    keras.layers.Reshape([7, 7, 128]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\")\n",
    "])\n",
    "\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(\n",
    "        64, \n",
    "        kernel_size=5, \n",
    "        strides=2, \n",
    "        padding=\"same\", \n",
    "        activation=keras.layers.LeakyReLU(0.2),\n",
    "        input_shape=[28, 28, 1]\n",
    "    ),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Conv2D(\n",
    "        64, \n",
    "        kernel_size=5, \n",
    "        strides=2, \n",
    "        padding=\"same\", \n",
    "        activation=keras.layers.LeakyReLU(0.2)\n",
    "    ),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "# discriminator needs input of [28, 28, 1]\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) * 2. - 1. # rescale to work with the tanh -1 to 1 range\n",
    "\n",
    "# can call it with the same code as earlier\n",
    "train_gan(gan, dataset, batch_size, codings_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
