{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 2 2 3 2 3 3 2 3 2 3 2 2 3 2 3 2 3 2 2\n",
      " 2 2 2 2 2 3 3 3 3 2 2 2 2 2 3 3 2 2 3 3 3 3 3 2 3 3 4 0 4 0 4 4 2 4 4 4 0\n",
      " 0 0 0 0 0 0 4 4 2 0 0 4 2 0 4 2 2 0 4 4 4 0 2 0 4 0 0 2 0 0 0 0 4 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 5 clusters\n",
    "# output is an array of ints where each int \n",
    "# is the cluster it belongs to in this 0-4\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris[\"data\"][:, (2,3)]\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([[0, 2], [3,2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning each data point to a cluster is hard clustering\n",
    "# we can also give each point a score this is called soft clustering\n",
    "\n",
    "# transform measures distance from the point to each clusters mean\n",
    "kmeans.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a rough idea of where centroids should be just set run it with that and n_init as 1\n",
    "\n",
    "good_init = np.array([[-3,3], [-3, 2]])\n",
    "kmeans = KMeans(n_clusters=2, init=good_init, n_init=1)\n",
    "x_new = np.array([[0, 2], [3,2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.fit_predict(x_new)\n",
    "kmeans.predict(x_new)\n",
    "# the best way tho is to init randomly and pick the best one, n_init is 10 by default\n",
    "# the best one is the one with the lowest inertia, which is the mse between each instance and it's centroid\n",
    "kmeans.inertia_\n",
    "# return negative inertia with score, it will try to maximize the score\n",
    "kmeans.score(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "                init_size=None, max_iter=100, max_no_improvement=10,\n",
       "                n_clusters=5, n_init=3, random_state=None,\n",
       "                reassignment_ratio=0.01, tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minibatch kmeans runs the algorithm with batches\n",
    "# if the dataset still doesn't fit in memory go back to dimensionality reduction and use memmap\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5)\n",
    "minibatch_kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score\n",
    "# (b-a) / max(a,b)\n",
    "# a is the mean distance to other points in the same cluster\n",
    "# b is mean distance to the instances of the nearest cluster\n",
    "# scale of -1 to +1 \n",
    "# +1 means it's in the middle of it's cluster and far from the middle of the other\n",
    "# -1 means the opposite and that it has probably been assigned to the wrong one\n",
    "# plot each one over a range of k to get a silhouette diagram\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(x)\n",
    "\n",
    "silhouette_score(x, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compare the silhouette scores for different numbers of clusters with a silhouette diagram\n",
    "# The best choice is the one where every cluster has a silhouette score above the mean, meaning all its clusters are\n",
    "# distinct from each other\n",
    "\n",
    "import matplotlib\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(x.reshape(-1, 1))\n",
    "                for k in range(1, 10)]\n",
    "\n",
    "silhouette_scores = [silhouette_score(x.reshape(-1, 1), model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(x.reshape(-1, 1), y_pred)\n",
    "\n",
    "    padding = len(x) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = matplotlib.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    \n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code for image segmentation based on color\n",
    "# breaking the image into segments, assigning each pixel to a segment\n",
    "# ex: self driving cars use this for objectx detection\n",
    "\n",
    "# from matplotlib.image import imread\n",
    "# import os\n",
    "\n",
    "# image = imread(os.path.join(\"images\", \"unsupervised_learning\", \"ladybug.png\"))\n",
    "# image.shape\n",
    "\n",
    "# x = image.reshape(-1, 3)\n",
    "# kmeans = KMeans(n_clusters=8).fit(x)\n",
    "# segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "# segmented_img = segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "x_digits, y_digits = load_digits(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_digits, y_digits)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "log_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use clustering for pre processing and dimensionality reduction\n",
    "# using an arbitrary number of clusters here:\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=50)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "pipeline.fit(x_train, y_train)\n",
    "pipeline.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use gridsearch to find the best number of clusters to use in the preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(kmeans__n_clusters=range(2, 100))\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\n",
    "grid_clf.fit(x_train, y_train)\n",
    "\n",
    "print(grid_clf.best_params_)\n",
    "print(grid_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use clustering for semi supervised learning\n",
    "\n",
    "n_labeled = 50\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train[:n_labeled], y_train[:n_labeled])\n",
    "\n",
    "log_reg.score(x_test, y_test)\n",
    "\n",
    "# use kmeans to find the representative images which are closest to the centroids\n",
    "# we will use these to classify the other unlabeled images\n",
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "x_digits_dist = kmeans.fit_transform(x_train)\n",
    "representative_digit_idx = np.argmin(x_digits_dist, axis=0)\n",
    "x_representative_digits = x_train[representative_digit_idx]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "for index, x_representative_digit in enumerate(x_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(x_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can manually label a section of images, and use that as the labels for our training data\n",
    "y_representative_digits = np.array([ \n",
    "    3, 9, 2, 6, 7, 9, 0, 2, 1, 6,\n",
    "    4, 9, 4, 2, 0, 5, 1, 4, 7, 5,\n",
    "    3, 2, 7, 3, 5, 9, 6, 9, 1, 7, \n",
    "    2, 8, 8, 5, 4, 5, 7, 8, 3, 0,\n",
    "    6, 1, 5, 2, 9, 8, 8, 8, 6, 1\n",
    "])\n",
    "\n",
    "# and use it to train a classifier\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_representative_digits, y_representative_digits)\n",
    "log_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can improve on this by propagating the labels to all the instances in the same cluster\n",
    "\n",
    "y_train_propagated = np.empty(len(x_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n",
    "    \n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train_propagated)\n",
    "log_reg.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better idea is to propagate to only the labels closest to the centroids, not the ones on the boundaries\n",
    "\n",
    "percentile_closest = 20\n",
    "\n",
    "x_cluster_dist = x_digits_dist[np.arange(len(x_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = x_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (x_cluster_dist > cutoff_distance)\n",
    "    x_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "partially_propagated = (x_cluster_dist != -1)\n",
    "x_train_partially_propagated = x_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN algorithm\n",
    "# for each instance it counts how many instances located a certain distance from it\n",
    "# If an instance has at least min_samples from it it is a core instance\n",
    "# All instances in the same neighborhood of a core instance belong to its cluster\n",
    "# Anything that isn't a core instance or isn't a neighborhood of one is an anomaly\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x, y = make_moons(n_samples=1000, noise=0.05)\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(x)\n",
    "\n",
    "# the ones that have -1 don't any cluster and are anomalies\n",
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan doesn't have a predict method for new instances only fit_predict\n",
    "# if you want to predict new instances u need to implement something else\n",
    "# we can use knn on new instances and add them to our dbscan clusters\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
    "\n",
    "x_new = np.array([[-0.5, 0],[0, 0.5], [1,-0.1], [2,1]])\n",
    "print(knn.predict(x_new))\n",
    "print(knn.predict_proba(x_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make this KNeighbors classifier account for outliers with a max distance field\n",
    "\n",
    "y_dist, y_pred_idx = knn.kneighbors(x_new, n_neighbors=1)\n",
    "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
    "y_pred[y_dist > 0.2] = -1\n",
    "\n",
    "y_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mixtures, clusters formed from a distribution based on the input data\n",
    "# given a dataset X first start by estimating the weights and distribution parameters\n",
    "# GM is like a KM but it also finds the size, shape and orientation of the clusters, not just the means\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=3, n_init=10)\n",
    "gm.fit(x)\n",
    "\n",
    "print(gm.weights_)\n",
    "print(gm.means_)\n",
    "print(gm.covariances_)\n",
    "print(gm.converged_)\n",
    "print(gm.n_iter_)\n",
    "print(gm.predict(x))\n",
    "print(gm.predict_proba(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since GM's generate a distribution you can get new instances by sampling them\n",
    "x_new, y_new = gm.sample(6)\n",
    "print(x_new)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can estimate the density of the function at any point, aka how dense the cluster will be\n",
    "\n",
    "print(gm.score_samples(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can constrain it's shape by setting the covariance_type hyperparameter\n",
    "#gm = GaussianMixture(n_components=3, n_init=10, covariance_type=\"spherical\") must be spheres\n",
    "#gm = GaussianMixture(n_components=3, n_init=10, covariance_type=\"diag\") ellipsoid must be parallel to coordinate axes\n",
    "#gm = GaussianMixture(n_components=3, n_init=10, covariance_type=\"tied\") all clusters must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm's are the best for anomaly detection\n",
    "# identifying outliers with a 4% defective threshold\n",
    "\n",
    "densities = gm.score_samples(x)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = x[densities < density_threshold]\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't find the number of clusters with inertia or silhouette\n",
    "# you have use either Bayesian information criterion BIC = log(m)p - 2log(L) \n",
    "# or Akaike information criterion AIC = 2p - 2log(L)\n",
    "# m = # of instances\n",
    "# p = # of parameters being learned by the model\n",
    "# L = maximized likelihood\n",
    "\n",
    "print(gm.bic(x))\n",
    "print(gm.aic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian gaussian mixture helps select the number of clusters\n",
    "# set n_clusters higher than you think is necessary and automatically gets rid of the unnecessary ones\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "bgm = BayesianGaussianMixture(n_components=10, n_init=10)\n",
    "bgm.fit(x)\n",
    "print(np.round(bgm.weights_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "        1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the anomaly score of each sample using the IsolationForest algorithm\n",
    "\n",
    "# The IsolationForest ‘isolates’ observations by randomly selecting a feature\n",
    "# and then randomly selecting a split value between the maximum and minimum values\n",
    "# of the selected feature. Since recursive partitioning can be represented\n",
    "# by a tree structure, the number of splittings required to isolate a sample\n",
    "# is equivalent to the path length from to the path length from the root node\n",
    "# to the terminating node. This path length, averaged over a forest of such random trees \n",
    "# is a measure of normality and our decision function. Random partitioning produces\n",
    "# noticeably shorter paths for anomalies. Hence, when a forest of random trees\n",
    "# collectively produce shorter path lengths for particular samples, \n",
    "# they are highly likely to be anomalies.\n",
    "\n",
    "# Essentially it splits the space up with random bisection trees\n",
    "# until it can completely isolate the instance, if it is easier to isolate \n",
    "# (lower node depth in the tree) then it is likely to be an anomaly\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iForest = IsolationForest(n_estimators=100,  contamination=0.1)\n",
    "iForest.fit_predict(x)\n",
    "# -1 are classified as anomalies, can change sensitivity by changing contamination parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1 -1  1  1 -1  1 -1  1  1  1  1 -1 -1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1 -1  1  1 -1  1 -1  1  1 -1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1\n",
      "  1  1 -1 -1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1 -1  1  1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# The anomaly score of each sample is called Local Outlier Factor. \n",
    "# It measures the local deviation of density of a given sample with respect to its neighbors. \n",
    "# It is local in that the anomaly score depends on how isolated the object is with respect \n",
    "# to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, \n",
    "# whose distance is used to estimate the local density. By comparing the local density of a sample \n",
    "# to the local densities of its neighbors, one can identify samples that have a substantially \n",
    "# lower density than their neighbors. These are considered outliers.\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=2)\n",
    "print(clf.fit_predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00592232e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -5.00000001e+08 -1.00000000e+00 -1.00000000e+00\n",
      " -5.00000001e+08 -1.41421356e+00 -5.00000001e+08 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -9.14213562e-01 -1.00000000e+09\n",
      " -1.00000000e+09 -1.00000000e+00 -1.45710678e+00 -1.06066017e+00\n",
      " -1.50000000e+00 -1.00000000e+00 -5.00000001e+08 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -5.00000001e+08 -1.00000000e+00 -1.00000000e+00 -5.00000001e+08\n",
      " -1.00000000e+00 -5.00000001e+08 -1.00000000e+00 -1.00000000e+00\n",
      " -5.00000001e+08 -5.00000001e+08 -1.00000000e+00 -1.42258898e+00\n",
      " -1.32842712e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+09 -1.00000000e+00\n",
      " -1.10355339e+00 -1.00000000e+00 -1.00000000e+00 -1.41421356e+09\n",
      " -1.00000000e+00 -2.00000000e+00 -1.25599806e+00 -1.00000000e+00\n",
      " -2.05746779e+00 -1.00000000e+00 -1.00000000e+00 -1.41257038e+00\n",
      " -1.00000000e+00 -1.00592232e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.70710678e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.29289322e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.06066017e+00 -9.95812289e-01 -9.14213562e-01 -9.26776695e-01\n",
      " -1.00000000e+00 -7.50000001e+08 -1.10355339e+00 -1.00000000e+00\n",
      " -5.00000001e+08 -1.00000000e+00 -1.20710678e+00 -1.00000000e+00\n",
      " -5.00000001e+08 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -1.58113883e+00 -5.00000001e+08\n",
      " -1.02950850e+00 -1.00000000e+00 -1.10355339e+00 -1.00000000e+00\n",
      " -1.08578644e+00 -1.17157288e+00 -7.50000001e+08 -1.02786404e+00\n",
      " -1.70710678e+00 -1.02950850e+00 -9.14213562e-01 -1.42258898e+00\n",
      " -1.12132034e+00 -1.10355339e+00 -1.10355339e+00 -1.25000000e+00\n",
      " -1.00000000e+00 -9.26776695e-01 -1.71097067e+00 -1.00000000e+00\n",
      " -1.08578644e+00 -1.10355339e+00 -9.26776695e-01 -5.00000001e+08\n",
      " -9.26776695e-01 -9.38659202e-01 -1.00000000e+00 -5.00000001e+08\n",
      " -9.02368927e-01 -1.70710678e+00 -9.73606798e-01 -1.05901699e+00\n",
      " -9.02368927e-01 -1.10355339e+00 -2.41421356e+00 -1.05792239e+00\n",
      " -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.12132034e+00\n",
      " -1.00000000e+00 -1.10355339e+00 -1.00000000e+00 -1.03033009e+00\n",
      " -1.00000000e+00 -7.47546896e-01 -1.00000000e+00 -1.18933983e+00\n",
      " -1.00000000e+00 -1.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, -1])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to use it for novelty detection \n",
    "# (checking if new data is an outlier after fitting)\n",
    "# you must set novelty = True\n",
    "clf = LocalOutlierFactor(n_neighbors=2, novelty=True, contamination=0.15)\n",
    "clf.fit(x)\n",
    "x_new = np.array([[-0.5, 0],[0, 0.5], [1,-0.1], [2,1]])\n",
    "# this shows the normality of the data, inliers here ~-1, outliers will be < -1\n",
    "print(clf.negative_outlier_factor_)\n",
    "clf.predict(x_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
