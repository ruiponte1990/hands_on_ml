{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019712493"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2, = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "x_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "x_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "x_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "# naive forecasting predicting the last value in the series\n",
    "\n",
    "y_pred = x_valid[:, -1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple rnn\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(1, input_shape=[None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep rnn\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "# switching to dense for the final, performs just as well and converges faster\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code above was for forecasting the next step, now for predicting several steps ahead\n",
    "# here we use the already trained model, predict the next, add it to previous, and then use that to predict the next...\n",
    "series = generate_time_series(1, n_steps + 10)\n",
    "x_new, y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "x = x_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(x[:, step_ahead:])[:, np.newaxis, :]\n",
    "    x = np.concatenate([x, y_pred_one], axis=1)\n",
    "y_pred = x[:, n_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can train an rnn to predict the next 10 steps ahead at once\n",
    "\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "x_train, y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "x_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "x_test, y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10) # the change here\n",
    "])\n",
    "\n",
    "y_pred = model.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now instead of predicting the next 10 at the last step, \n",
    "# we are going to predict the next 10 at every step\n",
    "# going from sequence --> vector to sequence --> sequence\n",
    "# each sequence has to be the same length as the input, with a 10-D vector at every step\n",
    "\n",
    "y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps]\n",
    "y_train = y[:7000]\n",
    "y_valid = y[7000:9000]\n",
    "y_test = y[9000:]\n",
    "\n",
    "# return sequences must be true at every step in the model\n",
    "# output a dense layer at every step\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10)) # we could have just done Dense()\n",
    "])\n",
    "\n",
    "# mse over all outputs for training, but for evaluation only want the last step\n",
    "def last_time_step_mse(y_true, y_pred):\n",
    "    return keras.metrics.mean_squared_error(y_true[:, -1], y_pred[:, -1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "# combine this technique with the concatenate loop above to produce arbitrarily long sequences\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization, normalize data within a layers features, not all the feature input coming in\n",
    "\n",
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization() # normalize before the activation!\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs)) # normalize before activation\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the custom layer\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "# alternative  but the one above can run on gpu so it's better\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve the performance we can shorten the input sequences with 1-D convolutions\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\"),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10)),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(x_train, y_train[:, 3::2], epochs=20, validation_data=(x_valid, y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavenet, lower layers learn short term patterns, deeper layers learn long term patterns\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1,2,3,4,8) * 2:\n",
    "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"casual\", activation=\"relu\", dilation_rate=rate))\n",
    "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
