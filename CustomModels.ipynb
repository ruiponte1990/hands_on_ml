{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "arr = tf.constant([[1.,2.,3.],[4.,5.,6.]])\n",
    "num = tf.constant(42)\n",
    "print(arr.shape)\n",
    "print(num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(arr[:, 1:])\n",
    "print(arr[..., 1, tf.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(arr+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.square(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.transpose(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11. 26.]\n",
      " [14. 35.]\n",
      " [19. 46.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# keras has a low level api too\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "k = keras.backend\n",
    "print(k.square(k.transpose(arr)) + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float32)\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64)\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n"
     ]
    }
   ],
   "source": [
    "# you can use tensors with numpy\n",
    "# np uses 64 bit, but tf uses 32 bit, so when converting set the dtype\n",
    "import numpy as np\n",
    "\n",
    "arr2 = np.array([2.,4.,5.])\n",
    "print(tf.constant(arr2, dtype=tf.float32))\n",
    "print(arr.numpy())\n",
    "print(tf.square(arr2))\n",
    "print(np.square(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type conversions are expensive and not automatic, will throw an exception if you don't do it manually\n",
    "\n",
    "tf.constant(2.) + tf.constant(40, dtype=tf.float32)\n",
    "\n",
    "# or can do a cast\n",
    "x = tf.constant(40, dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(x, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of these are constants and can't be used in a neural net, for that we need variables\n",
    "\n",
    "v = tf.Variable([[1.,2.,3.],[4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying a variable\n",
    "v.assign(2 * v)\n",
    "v[0, 1].assign(42)\n",
    "v[:, 2].assign([0., 1.])\n",
    "# updating multiple values at once with scatter\n",
    "v.scatter_nd_update(indices=[[0,0],[1,2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom huber loss fn, quadratic when small, linear when big, so it's good for outliers\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# now you can pass this loss into your model\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# when you load this model again though all its custom objects will need to be mapped\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a configured loss function\n",
    "\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
    "\n",
    "# specify the threshold when you reload the model, it isn't saved\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can solve this by subclassing keras.loss.Loss and implementing get_config\n",
    "\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\" : self.threshold}\n",
    "\n",
    "    \n",
    "# use it the same way\n",
    "\n",
    "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")\n",
    "\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"HuberLoss\": HuberLoss)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activations, initializers, regularizers, and constraints\n",
    "\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z)+1.0)\n",
    "def my_glorot_initializer(shaper, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2./ (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "# arguments depend on the function\n",
    "\n",
    "layer = keras.layers.Dense(30, \n",
    "                           activation=my_softplus, \n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)\n",
    "\n",
    "# if you want to save the hyperparameters you need to subclass the class\n",
    "# call for losses, layers, models\n",
    "# __call__ for regularizers, initializers, constraints\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\":self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use our custom classes as metrics\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "\n",
    "# example precision metric\n",
    "\n",
    "precision = keras.metrics.Precision()\n",
    "\n",
    "# can pass it labels and predictions\n",
    "\n",
    "precision([0,1,1,1,0,1,0,1],[1,1,0,1,0,1,0,1]) # would return .8\n",
    "precision([0,1,0,0,1,0,1,1],[1,0,1,1,0,0,0,0]) # now it would return .5\n",
    "\n",
    "# streaming metric it returns the overall precision of everything passed to it\n",
    "# when you are ready you can call it\n",
    "precision.result()\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom metric\n",
    "\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\") # add_weight creates variables needed\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\") # to keep track of the state over many batches\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None): # update_state used when you use an instance of \n",
    "        metric = self.huber_fn(y_true, y_pred)                  # the class as a function. updates variables for batch\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self): # computes and returns final result\n",
    "        return self.total / self.count\n",
    "    def get_config(self): # ensures model is saved correctly\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple custom layer that applies exp function to its inputs\n",
    "\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of dense layer\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight( # builds the layers variables with add_weight\n",
    "            name=\"kernel\", \n",
    "            shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=[self.units],\n",
    "            initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, x): # performs the calculation, returns the result\n",
    "        return self.activation(x @ self.kernel + self.bias) # the @ here is a matrix multiplication operation\n",
    "    def compute_output_shape(self, batch_input_shape): # returns shape of the layers outputs\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of layer with multiple inputs\n",
    "\n",
    "class MyMultiLayer(keras.layers.Layers):\n",
    "    def call(self, x): # since this is multi-input, the param is a tuple\n",
    "        x1, x2 = x\n",
    "        return [x1+x2, x1*x2, x1/x2] # returns the list of output\n",
    "    def compute_output_shape(self, batch_input_shape): # returns list of output shapes\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want different behavior during training and testing, add a training parameter\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(x), stddev=self.stddev)\n",
    "            return x + noise\n",
    "        else:\n",
    "            return x\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can build models with loops and skip connections\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs):\n",
    "            self.hidden = [keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")\n",
    "                           for _ in range(n_layers)]\n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.hidden:\n",
    "            z = layer(z)\n",
    "        return inputs + z\n",
    "    \n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = self.hidden1(inputs)\n",
    "        for _ in range(1+3):\n",
    "            z = self.block1(z)\n",
    "        z = self.block2(z)\n",
    "        return self.out(z)\n",
    "    \n",
    "    # we would need to implement get_config for saving and loading the weights of a trained version of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss based on model internals\n",
    "\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30,activation=\"selu\",kernel_initializer=\"lecun_normal\")\n",
    "                           for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.hidden:\n",
    "            z = layer(z)\n",
    "        reconstruction = self.reconstruct(z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs)) # Computes the mean of elements across dimensions of a tensor.\n",
    "        # when we define a custom loss pass it to add_loss\n",
    "        # for a custom metric do the same and pass it to add_metric\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(z)\n",
    "\n",
    "def f(w1,w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2+2 * w1 * w2\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.) # making our variables\n",
    "with tf.GradientTape() as tape: # records a tape of every computation\n",
    "    z = f(w1,w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2]) # gets the gradient, can only call once since it deletes the tape\n",
    "\n",
    "# can make the tape persistent\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1,w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "\n",
    "del tape\n",
    "\n",
    "c1, c2 = tf.Constant(5.), tf.Constant(3.) # making our variables\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1,c2)\n",
    "gradients = tape.gradient(z, [c1, c2]) # [None, None] cannot use the tape with constant\n",
    "\n",
    "# unless you tell it to watch them\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1,c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2]) # this works\n",
    "\n",
    "# if you want to stop the gradients from backpropagating\n",
    "\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradients(2*w1*w2) # stop_gradient \n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the gradients of softplus you can run into issues\n",
    "# you can avoid these by implementing a custom one\n",
    "\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also write a custom training method if fit() doesn't do what you want\n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, \n",
    "                       activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "                      \n",
    "])\n",
    "\n",
    "# sampling function\n",
    "\n",
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(x_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# building our custom training loop\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps+1)\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss]+model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # get the gradients and use for descent\n",
    "        for variabel in model.variables: # this is if you apply constraints to the model\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step*batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all these do the same\n",
    "\n",
    "def cube(x):\n",
    "    return x**3\n",
    "\n",
    "cube(tf.constant(2.0))\n",
    "\n",
    "tf_cube=tf.function(cube)\n",
    "\n",
    "tf_cube(2)\n",
    "\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# can call the original python function\n",
    "tf_cube.python_function(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
