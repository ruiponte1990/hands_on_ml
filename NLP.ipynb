{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char vs word level encoding\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "tokenizer.texts_to_sequences([\"First\"])\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "# encoding the full text so each char is mapped to an id\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1\n",
    "\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# cut into smaller sequences to feed the net for training\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# flatten list of lists to get ready to feed to net\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length)) # creating a list of tensors\n",
    "\n",
    "# shuffle\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "#encoding\n",
    "\n",
    "dataset = dataset.map(lambda x_batch, y_batch : (tf.one_hot(x_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    x = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(x, max_id)\n",
    "\n",
    "x_new = preprocess([\"How are yo\"])\n",
    "y_pred = model.predict_classes(x_new)\n",
    "tokenizer.sequences_to_texts(y_pred + 1)[0][-1]\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    x_new = preprocess([text])\n",
    "    y_proba = model.predict(x_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))\n",
    "print(complete_text(\"w\", temperature=1))\n",
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously were all stateless, they don't save their state after processing\n",
    "# now moving to stateful, must use sequential and non overlapping sequences\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True) #vs earlier where shift was 1\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1) # batch with a single window to avoid overlap/consecutive problem\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, \n",
    "                     return_sequences=True, \n",
    "                     stateful=True, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2, \n",
    "                     batch_input_shape=[batch_size, None, max_id]\n",
    "                    ),\n",
    "    keras.layers.GRU(128, \n",
    "                     return_sequences=True, \n",
    "                     stateful=True, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2\n",
    "                    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# need to reset the states for each epoch before we go back to the beginning of the text\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "\n",
    "# loading the data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# visualizing the words\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in x_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "\n",
    "# formatting the data\n",
    "\n",
    "def preprocess(x_batch, y_batch):\n",
    "    x_batch = tf.strings.substr(x_batch, 0, 300)\n",
    "    x_batch = tf.strings.regex_replace(x_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    x_batch = tf.strings.regex_replace(x_batch, b\"[^a-zA-Z]\", b\" \")\n",
    "    x_batch = tf.strings.split(x_batch)\n",
    "    return x_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# making a vocabulary\n",
    "\n",
    "vocabulary = Counter()\n",
    "for x_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in x_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "# we only need 10000 most common\n",
    "\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary =[ word for word, count in vocabulary.most_common()[:vocab_size] ]\n",
    "\n",
    "# replace words with id using pre process function\n",
    "\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "def encode_words(x_batch, y_batch):\n",
    "    return table.lookup(x_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([ # mask_zero = True teaches it to ignore the padding tokens, aka the zero index\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True, mask_zero=True),\n",
    "    keras.layers.GRU(128, mask_zero=True),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\", mask_zero=True)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another masking example\n",
    "\n",
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs,0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre trained model, can cache by setting environment variable \"TFHUB_CACHE_DIR\"\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", # sentence encoder\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),    # parses each string and makes \n",
    "    keras.layers.Dense(128, activation=\"relu\"),                            # a matrix of the word \n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# load the imdb reviews data\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine translation with encoder-decoder\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSample()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths\n",
    ")\n",
    "y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional nn, predict forwards and backwards\n",
    "# just add this layer to the model\n",
    "\n",
    "keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search run n = beam_width copies of the model in parallel to predict the best sequence\n",
    "# sub this decoder in for the basic decoder above\n",
    "\n",
    "beam_width = 10\n",
    "decoder = tfa.seq2seq.beam_search_decoder(\n",
    "    cell = decoder_cell,\n",
    "    beam_width = beam_width,\n",
    "    output_layer = output_layer\n",
    ")\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
    "    encoder_state,\n",
    "    multiplier=beam_width\n",
    ")\n",
    "ouputs, _, _ = decoder(\n",
    "    embedding_decoder,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=end_token, # token that signals start of sentence\n",
    "    initial_state=decoder_initial_state # token that is the end of the sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding luong attention to an encoder-decoder\n",
    "# use for long sequences so the model doesn't forget important tokens\n",
    "\n",
    "attention_mechanisms = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
    "    units,\n",
    "    encoder_state,\n",
    "    memory_sequence_length=encoder_sequence_length\n",
    ")\n",
    "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "    decoder_cell,\n",
    "    attention_mechanism,\n",
    "    attention_layer_size=n_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "\n",
    "class PositionalEncoding(keras.layers.layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims +=1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "    \n",
    "# creating the first layers of the transformer/attention architechture\n",
    "\n",
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embeddings(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras attention layers\n",
    "\n",
    "z = encoder_in\n",
    "for N in range(6):\n",
    "    z = keras.layers.Attention(use_scale=True)([z,z])\n",
    "\n",
    "encoder_outputs = z\n",
    "z = decoder_in\n",
    "for N in range(6):\n",
    "    z = keras.layers.Attention(use_scale=True, casual=True)([z,z])\n",
    "    z = keras.layers.Attention(use_scale=True)([z, encoder_outputs])\n",
    "    \n",
    "outputs = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation=\"softmax\"))(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
