{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a simple linear perceptron\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris=load_iris()\n",
    "x = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0)\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(x,y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 0 2 7 2 5 5]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "# data comes split into train and test already\n",
    "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_valid, x_train = x_train_full[:5000] / 255.0, x_train_full[5000:] / 255.0 # / 255 to scale pixel values\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \n",
    "               \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n",
    "               \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(y_valid[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building the network\n",
    "\n",
    "model = keras.models.Sequential() # sequential the simplest kind of NN\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28])) # Flatten layer convets 2D image into 1D array\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) # Dense layers manage their own weight matrix for you and\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) # Compute W.x + b, these use ReLu activation function\n",
    "# Another dense layer for the output with softmax for final classfification 10 = # of classes\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) \n",
    "\n",
    "# alternatively we can pass all these into sequential as params\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.layers.core.Flatten object at 0x13e3bf240>, <tensorflow.python.keras.layers.core.Dense object at 0x1471aca20>, <tensorflow.python.keras.layers.core.Dense object at 0x13e417668>, <tensorflow.python.keras.layers.core.Dense object at 0x1471c8048>]\n",
      "dense_19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers)\n",
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "model.get_layer(hidden1.name) is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0692279  -0.06050552  0.01229084 ... -0.0580014   0.04952827\n",
      "  -0.01047085]\n",
      " [ 0.02382818  0.03537996 -0.04347699 ...  0.06091744  0.01998483\n",
      "   0.0648745 ]\n",
      " [-0.00396203 -0.00791813 -0.01363686 ...  0.03378983  0.05757326\n",
      "  -0.02738904]\n",
      " ...\n",
      " [ 0.00571486 -0.03526743  0.05519749 ...  0.03107399  0.05601481\n",
      "  -0.03156022]\n",
      " [ 0.05448185 -0.05182195 -0.00108042 ...  0.06774092  0.06570348\n",
      "  -0.04225105]\n",
      " [ 0.00175667  0.02780417 -0.06093802 ...  0.00289954  0.06947821\n",
      "  -0.02149503]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you create your models architechture, use model.compile to specify the loss function, optimizer and metric\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different losses for different situations\n",
    "# sparse_categorical_crossentropy for sparse vectors ie [1, 2, 3, 4, 5] as labels\n",
    "# use categorical_crossentropy for 1 hot vectors [[1,0,0],[0,1,0]]\n",
    "# for binary classification go back and use \"sigmoid\" activation and \"binary_crossentropy\" as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 5s 89us/sample - loss: 0.7191 - accuracy: 0.7657 - val_loss: 0.4989 - val_accuracy: 0.8382\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.4885 - accuracy: 0.8301 - val_loss: 0.4410 - val_accuracy: 0.8470\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.4439 - accuracy: 0.8446 - val_loss: 0.4118 - val_accuracy: 0.8590\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.4166 - accuracy: 0.8532 - val_loss: 0.3927 - val_accuracy: 0.8654\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.3957 - accuracy: 0.8617 - val_loss: 0.3794 - val_accuracy: 0.8728\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3791 - accuracy: 0.8662 - val_loss: 0.3787 - val_accuracy: 0.8696\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3670 - accuracy: 0.8697 - val_loss: 0.3876 - val_accuracy: 0.8654\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 3s 62us/sample - loss: 0.3562 - accuracy: 0.8735 - val_loss: 0.3556 - val_accuracy: 0.8738\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3459 - accuracy: 0.8772 - val_loss: 0.3487 - val_accuracy: 0.8780\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3368 - accuracy: 0.8799 - val_loss: 0.3567 - val_accuracy: 0.8758\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 63us/sample - loss: 0.3284 - accuracy: 0.8836 - val_loss: 0.3412 - val_accuracy: 0.8790\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.3204 - accuracy: 0.8866 - val_loss: 0.3392 - val_accuracy: 0.8794\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3122 - accuracy: 0.8888 - val_loss: 0.3534 - val_accuracy: 0.8730\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.3064 - accuracy: 0.8906 - val_loss: 0.3204 - val_accuracy: 0.8876\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.2993 - accuracy: 0.8923 - val_loss: 0.3248 - val_accuracy: 0.8864\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 62us/sample - loss: 0.2940 - accuracy: 0.8943 - val_loss: 0.3193 - val_accuracy: 0.8852\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 62us/sample - loss: 0.2883 - accuracy: 0.8958 - val_loss: 0.3177 - val_accuracy: 0.8882\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 62us/sample - loss: 0.2826 - accuracy: 0.8983 - val_loss: 0.3079 - val_accuracy: 0.8904\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 63us/sample - loss: 0.2776 - accuracy: 0.9001 - val_loss: 0.3441 - val_accuracy: 0.8704\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 64us/sample - loss: 0.2727 - accuracy: 0.9017 - val_loss: 0.3156 - val_accuracy: 0.8896\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 63us/sample - loss: 0.2668 - accuracy: 0.9036 - val_loss: 0.3118 - val_accuracy: 0.8874\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 4s 67us/sample - loss: 0.2637 - accuracy: 0.9042 - val_loss: 0.3170 - val_accuracy: 0.8852\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.2576 - accuracy: 0.9062 - val_loss: 0.3018 - val_accuracy: 0.8922\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 4s 67us/sample - loss: 0.2533 - accuracy: 0.9096 - val_loss: 0.3041 - val_accuracy: 0.8922\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 4s 67us/sample - loss: 0.2495 - accuracy: 0.9104 - val_loss: 0.3096 - val_accuracy: 0.8884\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 4s 68us/sample - loss: 0.2455 - accuracy: 0.9112 - val_loss: 0.3020 - val_accuracy: 0.8908\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 62us/sample - loss: 0.2414 - accuracy: 0.9124 - val_loss: 0.3236 - val_accuracy: 0.8872\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.2379 - accuracy: 0.9143 - val_loss: 0.3057 - val_accuracy: 0.8912\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 61us/sample - loss: 0.2337 - accuracy: 0.9158 - val_loss: 0.3054 - val_accuracy: 0.8908\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 4s 66us/sample - loss: 0.2305 - accuracy: 0.9169 - val_loss: 0.3035 - val_accuracy: 0.8914\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curves\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 47us/sample - loss: 57.1708 - accuracy: 0.8633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57.170831044572594, 0.8633]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on test for final performance\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can make predictions with the model\n",
    "x_new = x_test[:3]\n",
    "y_proba = model.predict(x_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ankle boot' 'Pullover' 'Trouser']\n"
     ]
    }
   ],
   "source": [
    "# to see the predicted classes\n",
    "y_pred = model.predict_classes(x_new)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.9598 - val_loss: 0.6512\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5770 - val_loss: 0.5010\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.5082 - val_loss: 0.4651\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4524 - val_loss: 0.4504\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4404 - val_loss: 0.4329\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4292 - val_loss: 0.4255\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4213 - val_loss: 0.4192\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4147 - val_loss: 0.4172\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4078 - val_loss: 0.4147\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4024 - val_loss: 0.4031\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3991 - val_loss: 0.4044\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3948 - val_loss: 0.3965\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3899 - val_loss: 0.3916\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3875 - val_loss: 0.3901\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3814 - val_loss: 0.3922\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3807 - val_loss: 0.4415\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3803 - val_loss: 0.3812\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3732 - val_loss: 0.3816\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3707 - val_loss: 0.3776\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3692 - val_loss: 0.3758\n",
      "5160/5160 [==============================] - 0s 24us/sample - loss: 0.3799\n"
     ]
    }
   ],
   "source": [
    "# regression MLP\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_valid, y_valid))\n",
    "mse_test = model.evaluate(x_test, y_test)\n",
    "x_new = x_test[:3] # pretending these are new instances\n",
    "y_pred = model.predict(x_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# wide and deep NN\n",
    "# Can learn complex patterns with the deep path, and simple ones with the short path\n",
    "# created here with functional API\n",
    "\n",
    "input_ = keras.layers.Input(shape=x_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 28, 28)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 28, 30)       870         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 28, 30)       930         dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 28, 58)       0           input_3[0][0]                    \n",
      "                                                                 dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 28, 1)        59          concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,859\n",
      "Trainable params: 1,859\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 3s 260us/sample - loss: 2.3424 - val_loss: 1.2292\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.9287 - val_loss: 0.7626\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.7174 - val_loss: 0.6769\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.6555 - val_loss: 0.6364\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.6219 - val_loss: 0.6073\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.5970 - val_loss: 0.5863\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.5768 - val_loss: 0.5677\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5604 - val_loss: 0.5519\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5462 - val_loss: 0.5386\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.5344 - val_loss: 0.5280\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5250 - val_loss: 0.5183\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5163 - val_loss: 0.5110\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5097 - val_loss: 0.5061\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5035 - val_loss: 0.4990\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4993 - val_loss: 0.4968\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 2s 180us/sample - loss: 0.4943 - val_loss: 0.4908\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 124us/sample - loss: 0.4911 - val_loss: 0.4881\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4902 - val_loss: 0.4842\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4844 - val_loss: 0.4819\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4822 - val_loss: 0.4786\n",
      "5160/5160 [==============================] - 0s 24us/sample - loss: 0.4970\n"
     ]
    }
   ],
   "source": [
    "# similar but here we want option to send subsets through wide vs deep path\n",
    "# needed to re run some earlier code, because changes seem to have been made to the model \n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "input_a = keras.layers.Input(shape=[5], name = \"wide_input\")\n",
    "input_b = keras.layers.Input(shape=[6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_b)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_a, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_a, input_b], outputs=[output])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "x_train_a, x_train_b = x_train[:, :5], x_train[:, 2:]\n",
    "x_valid_a, x_valid_b = x_valid[:, :5], x_valid[:, 2:]\n",
    "x_test_a, x_test_b = x_test[:, :5], x_test[:, 2:]\n",
    "\n",
    "x_new_a, x_new_b = x_test_a[:3], x_test_b[:3]\n",
    "\n",
    "history = model.fit((x_train_a, x_train_b), y_train, epochs=20, validation_data=((x_valid_a, x_valid_b), y_valid))\n",
    "mse_test = model.evaluate((x_test_a, x_test_b), y_test)\n",
    "y_pred = model.predict((x_new_a, x_new_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.856787 ],\n",
       "       [2.3069134],\n",
       "       [0.6789582]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 105us/sample - loss: 0.8745 - main_output_loss: 0.7363 - aux_output_loss: 2.1181 - val_loss: 0.5609 - val_main_output_loss: 0.5039 - val_aux_output_loss: 1.0724\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5542 - main_output_loss: 0.5054 - aux_output_loss: 0.9932 - val_loss: 0.4884 - val_main_output_loss: 0.4456 - val_aux_output_loss: 0.8727\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5519 - main_output_loss: 0.5194 - aux_output_loss: 0.8438 - val_loss: 0.4722 - val_main_output_loss: 0.4395 - val_aux_output_loss: 0.7651\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4990 - main_output_loss: 0.4703 - aux_output_loss: 0.7556 - val_loss: 0.4442 - val_main_output_loss: 0.4157 - val_aux_output_loss: 0.6999\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4860 - main_output_loss: 0.4611 - aux_output_loss: 0.7106 - val_loss: 0.4379 - val_main_output_loss: 0.4125 - val_aux_output_loss: 0.6654\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4769 - main_output_loss: 0.4539 - aux_output_loss: 0.6833 - val_loss: 0.4318 - val_main_output_loss: 0.4093 - val_aux_output_loss: 0.6336\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4635 - main_output_loss: 0.4414 - aux_output_loss: 0.6629 - val_loss: 0.4108 - val_main_output_loss: 0.3882 - val_aux_output_loss: 0.6131\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4516 - main_output_loss: 0.4306 - aux_output_loss: 0.6418 - val_loss: 0.4066 - val_main_output_loss: 0.3855 - val_aux_output_loss: 0.5948\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4563 - main_output_loss: 0.4367 - aux_output_loss: 0.6312 - val_loss: 0.3956 - val_main_output_loss: 0.3743 - val_aux_output_loss: 0.5864\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4348 - main_output_loss: 0.4147 - aux_output_loss: 0.6147 - val_loss: 0.3866 - val_main_output_loss: 0.3667 - val_aux_output_loss: 0.5650\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4278 - main_output_loss: 0.4087 - aux_output_loss: 0.5992 - val_loss: 0.3792 - val_main_output_loss: 0.3593 - val_aux_output_loss: 0.5573\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.4198 - main_output_loss: 0.4012 - aux_output_loss: 0.5874 - val_loss: 0.3740 - val_main_output_loss: 0.3556 - val_aux_output_loss: 0.5391\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4137 - main_output_loss: 0.3956 - aux_output_loss: 0.5763 - val_loss: 0.3733 - val_main_output_loss: 0.3558 - val_aux_output_loss: 0.5294\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4123 - main_output_loss: 0.3951 - aux_output_loss: 0.5662 - val_loss: 0.3642 - val_main_output_loss: 0.3469 - val_aux_output_loss: 0.5186\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4048 - main_output_loss: 0.3881 - aux_output_loss: 0.5548 - val_loss: 0.3703 - val_main_output_loss: 0.3544 - val_aux_output_loss: 0.5128\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4028 - main_output_loss: 0.3870 - aux_output_loss: 0.5443 - val_loss: 0.3552 - val_main_output_loss: 0.3391 - val_aux_output_loss: 0.4991\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3925 - main_output_loss: 0.3764 - aux_output_loss: 0.5374 - val_loss: 0.3501 - val_main_output_loss: 0.3343 - val_aux_output_loss: 0.4916\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3883 - main_output_loss: 0.3729 - aux_output_loss: 0.5271 - val_loss: 0.3507 - val_main_output_loss: 0.3360 - val_aux_output_loss: 0.4816\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3824 - main_output_loss: 0.3674 - aux_output_loss: 0.5161 - val_loss: 0.3424 - val_main_output_loss: 0.3274 - val_aux_output_loss: 0.4763\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3774 - main_output_loss: 0.3630 - aux_output_loss: 0.5080 - val_loss: 0.3483 - val_main_output_loss: 0.3347 - val_aux_output_loss: 0.4694\n",
      "5160/5160 [==============================] - 0s 32us/sample - loss: 0.3869 - main_output_loss: 0.3745 - aux_output_loss: 0.5057\n"
     ]
    }
   ],
   "source": [
    "# if we want our model to have different outputs we can do that too\n",
    "# for example if we want it to perform different classification tasks\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "input_a = keras.layers.Input(shape=[5], name = \"wide_input\")\n",
    "input_b = keras.layers.Input(shape=[6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_b)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_a, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_a, input_b], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
    "\n",
    "x_train_a, x_train_b = x_train[:, :5], x_train[:, 2:]\n",
    "x_valid_a, x_valid_b = x_valid[:, :5], x_valid[:, 2:]\n",
    "x_test_a, x_test_b = x_test[:, :5], x_test[:, 2:]\n",
    "x_new_a, x_new_b = x_test_a[:3], x_test_b[:3]\n",
    "\n",
    "# two different input data, but since both are predicting the same thing we can use the same label\n",
    "history = model.fit(\n",
    "[x_train_a, x_train_b], [y_train, y_train], epochs=20, validation_data=([x_valid_a, x_valid_b], [y_valid, y_valid])\n",
    ")\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate([x_test_a, x_test_b], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([x_new_a, x_new_b])\n",
    "# we can save our models as .h5 (HDF5) files\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to customize it you can just subclass it\n",
    "\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    # we can do customization in the call model, if statements, for loops anything we want in there\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_a, input_b = inputs\n",
    "        hidden1 = self.hidden1(input_b)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_a, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can pass a callback to the fit method\n",
    "# this checkpoints your model to the file,\n",
    "# you can also tell it to only save the \n",
    "# hyperparameters where it performed the best\n",
    "# implementing early stopping for you\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(x_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "# patience tells it how to long to continue after stopping with no improvement\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid), \n",
    "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can also write your own custom callback to pass to the models methods\n",
    "# callbacks for fit\n",
    "# on_train_begin\n",
    "# on_train_end\n",
    "# on_epoch_begin\n",
    "# on_epoch_end\n",
    "# on_batch_begin\n",
    "# on_batch_end\n",
    "# callbacks for evaluate\n",
    "# on_test_begin\n",
    "# on_test_end\n",
    "# on_test_batch_begin\n",
    "# on_test_batch_end\n",
    "# callbacks for predict\n",
    "# on_predict_begin\n",
    "# on_predict_end\n",
    "# on_predict_batch_begin\n",
    "# on_predict_batch_end\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 2.2664 - val_loss: 0.7296\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.7645 - val_loss: 0.6353\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6822 - val_loss: 0.6058\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6547 - val_loss: 0.5849\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.6338 - val_loss: 0.5658\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.6155 - val_loss: 0.5492\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5994 - val_loss: 0.5361\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5849 - val_loss: 0.5226\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5722 - val_loss: 0.5122\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5615 - val_loss: 0.5040\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5523 - val_loss: 0.4951\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5447 - val_loss: 0.4879\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5377 - val_loss: 0.4835\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5323 - val_loss: 0.4791\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5272 - val_loss: 0.4747\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5234 - val_loss: 0.4699\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5192 - val_loss: 0.4675\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5159 - val_loss: 0.4641\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5136 - val_loss: 0.4601\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5100 - val_loss: 0.4600\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5073 - val_loss: 0.4557\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5044 - val_loss: 0.4547\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5024 - val_loss: 0.4511\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5003 - val_loss: 0.4496\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4990 - val_loss: 0.4494\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4963 - val_loss: 0.4458\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4942 - val_loss: 0.4463\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4923 - val_loss: 0.4439\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4911 - val_loss: 0.4423\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4888 - val_loss: 0.4405\n"
     ]
    }
   ],
   "source": [
    "# tensorboard for visualization\n",
    "\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6358), started 0:12:45 ago. (Use '!kill 6358' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5f211dea3f884102\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5f211dea3f884102\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "test_logdir= get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100)+2) * step / 100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random images\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 1.4936 - val_loss: 0.6295\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.7000 - val_loss: 0.5723\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.6154 - val_loss: 0.5283\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5777 - val_loss: 0.5069\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5536 - val_loss: 0.4893\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5369 - val_loss: 0.4755\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5219 - val_loss: 0.4647\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5096 - val_loss: 0.4531\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5000 - val_loss: 0.4450\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4918 - val_loss: 0.4383\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4840 - val_loss: 0.4300\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4776 - val_loss: 0.4263\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4725 - val_loss: 0.4204\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4678 - val_loss: 0.4180\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4635 - val_loss: 0.4137\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4600 - val_loss: 0.4140\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4566 - val_loss: 0.4079\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4536 - val_loss: 0.4052\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4501 - val_loss: 0.4008\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4482 - val_loss: 0.3995\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4453 - val_loss: 0.3964\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4431 - val_loss: 0.3937\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4410 - val_loss: 0.3918\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4390 - val_loss: 0.3916\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4376 - val_loss: 0.3891\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4351 - val_loss: 0.3871\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4337 - val_loss: 0.3863\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4320 - val_loss: 0.3844\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4299 - val_loss: 0.3838\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4277 - val_loss: 0.3828\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4271 - val_loss: 0.3816\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4253 - val_loss: 0.3793\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4227 - val_loss: 0.3865\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4229 - val_loss: 0.3792\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4231 - val_loss: 0.3781\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4199 - val_loss: 0.3749\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4180 - val_loss: 0.3745\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4170 - val_loss: 0.3706\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4156 - val_loss: 0.3726\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4134 - val_loss: 0.3711\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4126 - val_loss: 0.3737\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4117 - val_loss: 0.3679\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4101 - val_loss: 0.3657\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4081 - val_loss: 0.3669\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4074 - val_loss: 0.3638\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4064 - val_loss: 0.3626\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4093 - val_loss: 0.3637\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4044 - val_loss: 0.3618\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4028 - val_loss: 0.3662\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4013 - val_loss: 0.3623\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4007 - val_loss: 0.3599\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3990 - val_loss: 0.3617\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3982 - val_loss: 0.3568\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3971 - val_loss: 0.3572\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3965 - val_loss: 0.3558\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3945 - val_loss: 0.3557\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3938 - val_loss: 0.3546\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3927 - val_loss: 0.3552\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3923 - val_loss: 0.3546\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3923 - val_loss: 0.3563\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3942 - val_loss: 0.3521\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3904 - val_loss: 0.3505\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3896 - val_loss: 0.3484\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3884 - val_loss: 0.3506\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3859 - val_loss: 0.3470\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3885 - val_loss: 0.3479\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3856 - val_loss: 0.3518\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4022 - val_loss: 0.3470\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3833 - val_loss: 0.3504\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3883 - val_loss: 0.3441\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3871 - val_loss: 0.3445\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3830 - val_loss: 0.3439\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3821 - val_loss: 0.3447\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3802 - val_loss: 0.3441\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3825 - val_loss: 0.3460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3808 - val_loss: 0.3412\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3798 - val_loss: 0.3425\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3780 - val_loss: 0.3400\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3779 - val_loss: 0.3406\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3761 - val_loss: 0.3398\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3806 - val_loss: 0.3399\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3787 - val_loss: 0.3369\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3720 - val_loss: 0.3388\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3753 - val_loss: 0.3387\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3824 - val_loss: 0.3378\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3738 - val_loss: 0.3624\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3792 - val_loss: 0.3347\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3762 - val_loss: 0.3342\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3693 - val_loss: 0.3342\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3709 - val_loss: 0.3369\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3741 - val_loss: 0.3338\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3695 - val_loss: 0.3404\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 0.3667 - val_loss: 0.3305\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 96us/sample - loss: 0.3677 - val_loss: 0.3329\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3664 - val_loss: 0.3323\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3729 - val_loss: 0.3598\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.3661 - val_loss: 0.3258\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 93us/sample - loss: 0.3628 - val_loss: 0.3291\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 107us/sample - loss: 0.3719 - val_loss: 0.3270\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 2s 133us/sample - loss: 0.3720 - val_loss: 0.3256\n",
      "5160/5160 [==============================] - 0s 19us/sample - loss: 0.3345\n"
     ]
    }
   ],
   "source": [
    "# build a mimic of an sklearn regressor\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# create a regressor with this function\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "keras_reg.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid), \n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    ")\n",
    "\n",
    "mse_test = keras_reg.score(x_test, y_test)\n",
    "y_pred = keras_reg.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3216228, 2.4811754, 1.094378 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\" : [0,1,2,3],\n",
    "    \"n_neurons\" : np.arange(1, 100),\n",
    "    \"learning_rate\" : reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
